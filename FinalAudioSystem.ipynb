{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee8b980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "from ipywidgets import IntProgress\n",
    "import scipy.io.wavfile\n",
    "import librosa\n",
    "from skimage.transform import resize\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import IPython.display as ipd\n",
    "import wave\n",
    "import pyaudio\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sounddevice as sd\n",
    "from IPython.display import clear_output\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27759ca",
   "metadata": {},
   "source": [
    "# Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a917cf",
   "metadata": {},
   "source": [
    "## 1.Gender and number recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc62ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1826d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(ModifiedResNet, self).__init__()\n",
    "                self.features = nn.Sequential(\n",
    "                    #remove last fully connected layer\n",
    "                    *list(resnet.children())[:-1]\n",
    "                )\n",
    "                self.dense1 = nn.Linear(512,128)\n",
    "                self.dense_number = nn.Linear(128,10)\n",
    "                self.dense_gender = nn.Linear(128,1)\n",
    "            def forward(self, x):\n",
    "              # input shape = batch_size,n_channels,height,width\n",
    "                x = self.features(x) # output shape = batch_size * 512 * 1 * 1\n",
    "                x = x.view(-1,512)\n",
    "                x = nn.ReLU()(self.dense1(x)) # ouput shape = batch_size * 128\n",
    "                number_pred = nn.Softmax()(self.dense_number(x))\n",
    "                gender_pred = nn.Sigmoid()(self.dense_gender(x))\n",
    "\n",
    "                return [number_pred,gender_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0eb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_number_recognition_model = ModifiedResNet()\n",
    "gender_number_recognition_model = gender_number_recognition_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab1ee76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_number_recognition_model.load_state_dict(torch.load(\".\\\\gender_recognition\\\\models\\\\ResNet\\\\best_model_both_gender_number_recognition\",map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93dfd2",
   "metadata": {},
   "source": [
    "## 2. Trigger word detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f1a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionModel(nn.Module):\n",
    "    \"\"\"The CNN model\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(DetectionModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64,\n",
    "                               kernel_size=(5, 5), bias=False,padding=\"same\")\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                               kernel_size=(3, 3), bias=False,padding=\"same\")\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128,\n",
    "                               kernel_size=(3, 3), bias=False)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 128, bias=True)\n",
    "        self.fc2 = nn.Linear(128, 1, bias=True)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        (_, time_len, mel_bins) = x.shape\n",
    "\n",
    "        x = x.view(-1, 1, time_len, mel_bins)\n",
    "        # print('Input')\n",
    "        # print(x.size())\n",
    "\n",
    "        x = nnF.relu(self.bn1(self.conv1(x)))\n",
    "        # print('Conv1')\n",
    "        # print(x.size())\n",
    "        x = nnF.max_pool2d(x,kernel_size=(2,4),padding=(0,2))\n",
    "        # print('Pool1')\n",
    "        # print(x.size())\n",
    "        \n",
    "        x = nnF.relu(self.bn2(self.conv2(x)))\n",
    "        # print('Conv2')\n",
    "        # print(x.size())\n",
    "        x = nnF.max_pool2d(x,kernel_size=(3,3),padding=(1,1))\n",
    "        # print('Pool2')\n",
    "        # print(x.size())\n",
    "        \n",
    "        x = nnF.relu(self.bn3(self.conv3(x)))\n",
    "        # print('Conv3')\n",
    "        # print(x.size())\n",
    "        x = nnF.max_pool2d(x,kernel_size=(5,4))\n",
    "        # print('Pool3')\n",
    "        # print(x.size())\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = nnF.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_and_convert(self, x):\n",
    "        \"Handles the torch<--->numpy tensor conversion, for convenience\"\n",
    "        x_torch = torch.FloatTensor(x)\n",
    "        y_torch = self.forward(x_torch)\n",
    "        return y_torch.detach().numpy()\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e258ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigger_word_detection_model = DetectionModel()\n",
    "trigger_word_detection_model.load_state_dict(torch.load(\".\\\\trigger\\\\simple_approach\\\\best_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c2cdc",
   "metadata": {},
   "source": [
    "## Define Audio preprocessing for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c400a4",
   "metadata": {},
   "source": [
    "### 1.word and gender detector model preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc68c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 48000\n",
    "sample_rate = 48000\n",
    "duration = 0.9999583333333333\n",
    "max_samples=47998\n",
    "kPRE_EMPHASIS_COEFF = 0.97\n",
    "def pad_signal(signal, target_len):\n",
    "    \n",
    "    num_zeros_needed = target_len - len(signal)\n",
    "    \n",
    "    if num_zeros_needed > 0:\n",
    "\n",
    "        num_zeros_front = np.random.randint(num_zeros_needed)\n",
    "        num_zeros_back = num_zeros_needed - num_zeros_front\n",
    "        return np.pad(signal, (num_zeros_front, num_zeros_back), mode='constant')\n",
    "\n",
    "    else:\n",
    "        return signal\n",
    "    \n",
    "def pre_emphasis(signal):\n",
    "    first_amp = signal[0]\n",
    "    all_amps_without_first = signal[1:]\n",
    "    all_amps_without_last = signal[:-1]\n",
    "    emphasized_signal = np.append(first_amp, all_amps_without_first - kPRE_EMPHASIS_COEFF * all_amps_without_last)\n",
    "    return emphasized_signal\n",
    "\n",
    "def pipeline(signal):\n",
    "    \n",
    "    emphasized_signal = pre_emphasis(signal)\n",
    "    # the following code applies dft, mel filter banks, logging, dct and normalization all at once\n",
    "    # truly convenient\n",
    "    lifted_mfcc = librosa.feature.mfcc(\n",
    "        y=emphasized_signal.astype(float), \n",
    "        sr=sample_rate, \n",
    "        n_mfcc=12, \n",
    "        dct_type=2, \n",
    "        norm='ortho', \n",
    "        lifter=22,\n",
    "        n_fft = int(sample_rate * 0.025),\n",
    "        hop_length= int(sample_rate * 0.01),\n",
    "        power=2,\n",
    "        center=False,\n",
    "        window='hanning',\n",
    "        n_mels=40\n",
    "    )\n",
    "\n",
    "    return lifted_mfcc\n",
    "\n",
    "def bytescale(data, cmin=None, cmax=None, high=255, low=0):\n",
    "    \"\"\"\n",
    "    Byte scales an array (image).\n",
    "\n",
    "    Byte scaling means converting the input image to uint8 dtype and scaling\n",
    "    the range to ``(low, high)`` (default 0-255).\n",
    "    If the input image already has dtype uint8, no scaling is done.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        PIL image data array.\n",
    "    cmin : scalar, optional\n",
    "        Bias scaling of small values. Default is ``data.min()``.\n",
    "    cmax : scalar, optional\n",
    "        Bias scaling of large values. Default is ``data.max()``.\n",
    "    high : scalar, optional\n",
    "        Scale max value to `high`.  Default is 255.\n",
    "    low : scalar, optional\n",
    "        Scale min value to `low`.  Default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_array : uint8 ndarray\n",
    "        The byte-scaled array.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> img = array([[ 91.06794177,   3.39058326,  84.4221549 ],\n",
    "                     [ 73.88003259,  80.91433048,   4.88878881],\n",
    "                     [ 51.53875334,  34.45808177,  27.5873488 ]])\n",
    "    >>> bytescale(img)\n",
    "    array([[255,   0, 236],\n",
    "           [205, 225,   4],\n",
    "           [140,  90,  70]], dtype=uint8)\n",
    "    >>> bytescale(img, high=200, low=100)\n",
    "    array([[200, 100, 192],\n",
    "           [180, 188, 102],\n",
    "           [155, 135, 128]], dtype=uint8)\n",
    "    >>> bytescale(img, cmin=0, cmax=255)\n",
    "    array([[91,  3, 84],\n",
    "           [74, 81,  5],\n",
    "           [52, 34, 28]], dtype=uint8)\n",
    "\n",
    "    \"\"\"\n",
    "    if data.dtype == np.uint8:\n",
    "        return data\n",
    "\n",
    "    if high < low:\n",
    "        raise ValueError(\"`high` should be larger than `low`.\")\n",
    "\n",
    "    if cmin is None:\n",
    "        cmin = data.min()\n",
    "    if cmax is None:\n",
    "        cmax = data.max()\n",
    "\n",
    "    cscale = cmax - cmin\n",
    "    if cscale < 0:\n",
    "        raise ValueError(\"`cmax` should be larger than `cmin`.\")\n",
    "    elif cscale == 0:\n",
    "        cscale = 1\n",
    "\n",
    "    scale = float(high - low) / cscale\n",
    "    bytedata = (data * 1.0 - cmin) * scale + 0.4999\n",
    "    bytedata[bytedata > high] = high\n",
    "    bytedata[bytedata < 0] = 0\n",
    "    return np.cast[np.uint8](bytedata) + np.cast[np.uint8](low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693eb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio):\n",
    "    audio = pad_signal(audio, target_len=48000)\n",
    "    mfc = pipeline(audio)\n",
    "    mfc_3d = resize(np.rollaxis(np.array([mfc] * 3), 0, 3), (224, 224, 3))\n",
    "    mfc_3d=bytescale(mfc_3d,cmin=0,cmax=255)\n",
    "    return mfc_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459741a2",
   "metadata": {},
   "source": [
    "### 2. trigger word detector model preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d752ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_melspectrogram(audio,sr=22050, win_len=0.05, hop_len=0.025, n_mels=64):\n",
    "#     audio, sr = librosa.load(\"{}\".format(filename), sr=22050)\n",
    "    win_len = int(win_len*sr)\n",
    "    hop_len = int(hop_len*sr)\n",
    "    spec = librosa.feature.melspectrogram(audio, sr, n_mels=n_mels, n_fft=2048, win_length=win_len, hop_length=hop_len)\n",
    "    return spec.transpose((1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae780c2",
   "metadata": {},
   "source": [
    "## Load case study datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c49285",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_MNIST_male_path = \".\\\\dataset\\\\gender_recognition_data\\\\male\\\\\"\n",
    "audio_MNIST_female_path = \".\\\\dataset\\\\gender_recognition_data\\\\female\\\\\"\n",
    "speech_command_dataset_path = \".\\\\dataset\\\\speech_command_dataset\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c7b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_number_audio_path = os.listdir(audio_MNIST_male_path)\n",
    "female_number_audio_path = os.listdir(audio_MNIST_female_path)\n",
    "speech_command_audio_path = os.listdir(speech_command_dataset_path)\n",
    "# make path varibales point to the full path\n",
    "male_number_audio_path = list(map(lambda male_path: audio_MNIST_male_path+male_path , male_number_audio_path))\n",
    "female_number_audio_path = list(map(lambda female_path: audio_MNIST_female_path+female_path , female_number_audio_path))\n",
    "speech_command_audio_path = list(map(lambda command_path: speech_command_dataset_path+command_path , speech_command_audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6c5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\"male saying a number\",\"female saying a number\",\"speech command dataset\"]\n",
    "audio_dict = {\"male saying a number\":male_number_audio_path,\"female saying a number\":female_number_audio_path,\"speech command dataset\":speech_command_audio_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079838d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d429f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to play audio\n",
    "chunk = 1024  \n",
    "def play_audio(audio_path):\n",
    "    #open a wav format music  \n",
    "    f = wave.open(audio_path,\"rb\") \n",
    "    p = pyaudio.PyAudio()  \n",
    "    #open stream  \n",
    "    stream = p.open(format = p.get_format_from_width(f.getsampwidth()),  \n",
    "                    channels = f.getnchannels(),  \n",
    "                    rate = f.getframerate(),  \n",
    "                    output = True)  \n",
    "    #read data  \n",
    "    data = f.readframes(chunk)  \n",
    "\n",
    "    #play stream  \n",
    "    while data:  \n",
    "        stream.write(data)  \n",
    "        data = f.readframes(chunk)  \n",
    "\n",
    "    #stop stream  \n",
    "    stream.stop_stream()  \n",
    "    stream.close()  \n",
    "\n",
    "    #close PyAudio  \n",
    "    p.terminate()  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc7ffcff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am listening, say the word 'on' to activate.\n",
      ". . \n",
      "Trigger word detected !!\n",
      "\n",
      "---------------------------------------------------\n",
      "number :  3\n",
      "gender : Female  \n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9736/3591183577.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"---------------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mplay_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchosen_audio_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[1;31m# model inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwavfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchosen_audio_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# faster than librosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9736/1538126627.py\u001b[0m in \u001b[0;36mplay_audio\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#play stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\pyaudio.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, frames, num_frames, exception_on_underflow)\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;31m#print len(frames), self._channels, self._width, num_frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m         pa.write_stream(self._stream, frames, num_frames,\n\u001b[0m\u001b[0;32m    586\u001b[0m                         exception_on_underflow)\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wait for activation word\n",
    "instructions = True\n",
    "while True:\n",
    "    if instructions:\n",
    "        print(\"I am listening, say the word 'on' to activate.\")\n",
    "        instructions= False\n",
    "    is_triggered = False\n",
    "    recording = sd.rec(int(1 * 22050), \n",
    "                   samplerate=22050, channels=1)\n",
    "  \n",
    "    # Record audio for the given number of seconds\n",
    "    sd.wait()\n",
    "    spectogram = extract_melspectrogram(recording[:,0])\n",
    "    with torch.no_grad():\n",
    "        pred = trigger_word_detection_model(torch.FloatTensor(spectogram[None,:]))\n",
    "    if pred >=0.9:\n",
    "        print()\n",
    "        print(\"Trigger word detected !!\")\n",
    "        is_triggered = True\n",
    "        print()\n",
    "    else:\n",
    "        print(\".\",end=\" \")\n",
    "    if is_triggered:\n",
    "        # main loop\n",
    "        for _ in range(5):\n",
    "            random_choice_idx = random.randint(0, 1)\n",
    "            choice = options[random_choice_idx]\n",
    "            candidate_audio = audio_dict[choice]\n",
    "            chosen_audio_path_idx = random.randint(0, len(candidate_audio)-1)\n",
    "            chosen_audio_path = candidate_audio[chosen_audio_path_idx]\n",
    "            print(\"---------------------------------------------------\")\n",
    "            time.sleep(2)#\n",
    "            play_audio(chosen_audio_path)\n",
    "            # model inference\n",
    "            sr, signal = scipy.io.wavfile.read(chosen_audio_path)  # faster than librosa\n",
    "            mfc_3d = preprocess_audio(signal)\n",
    "            mfc_3d = torch.tensor(mfc_3d.reshape((3,224,224)),dtype = torch.float32)\n",
    "            with torch.no_grad():\n",
    "                pred = gender_number_recognition_model(mfc_3d[None,:])\n",
    "\n",
    "            if random_choice_idx==2:\n",
    "                spoken_number=\"Not a number\"\n",
    "            else:\n",
    "                spoken_number = np.argmax(pred[0]).item()\n",
    "            if pred[1]>=0.5:\n",
    "                gender = \"Female\"\n",
    "                gender_pred_acc = pred[1].item()*100\n",
    "            else:\n",
    "                gender = \"Male\"\n",
    "                gender_pred_acc = (1-pred[1]).item()*100\n",
    "            print(\"number : \",spoken_number)\n",
    "            print(\"gender : {}  \".format(gender))\n",
    "            instructions = True\n",
    "        time.sleep(4)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e0bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679facd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19aade60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "gender : Female  \n",
      "---------------------------------------------------\n",
      "gender : Male  \n",
      "---------------------------------------------------\n",
      "gender : Female  \n",
      "---------------------------------------------------\n",
      "gender : Male  \n",
      "---------------------------------------------------\n",
      "gender : Female  \n",
      "---------------------------------------------------\n",
      "gender : Male  \n",
      "---------------------------------------------------\n",
      "gender : Male  \n",
      "---------------------------------------------------\n",
      "gender : Male  \n",
      "---------------------------------------------------\n",
      "gender : Female  \n",
      "---------------------------------------------------\n",
      "gender : Male  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(10):\n",
    "    random_choice_idx = 2\n",
    "    choice = options[random_choice_idx]\n",
    "    candidate_audio = audio_dict[choice]\n",
    "    chosen_audio_path_idx = random.randint(0, len(candidate_audio)-1)\n",
    "    chosen_audio_path = candidate_audio[chosen_audio_path_idx]\n",
    "    print(\"---------------------------------------------------\")\n",
    "    time.sleep(2)#\n",
    "    play_audio(chosen_audio_path)\n",
    "    # model inference\n",
    "    sr, signal = scipy.io.wavfile.read(chosen_audio_path)  # faster than librosa\n",
    "    mfc_3d = preprocess_audio(signal)\n",
    "    mfc_3d = torch.tensor(mfc_3d.reshape((3,224,224)),dtype = torch.float32)\n",
    "    with torch.no_grad():\n",
    "        pred = gender_number_recognition_model(mfc_3d[None,:])\n",
    "\n",
    "    if random_choice_idx==2:\n",
    "        spoken_number=\"Not a number\"\n",
    "    else:\n",
    "        spoken_number = np.argmax(pred[0]).item()\n",
    "    if pred[1]>=0.5:\n",
    "        gender = \"Female\"\n",
    "        gender_pred_acc = pred[1].item()*100\n",
    "    else:\n",
    "        gender = \"Male\"\n",
    "        gender_pred_acc = (1-pred[1]).item()*100\n",
    "    print(\"gender : {}  \".format(gender))\n",
    "    instructions = True\n",
    "time.sleep(4)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d18285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
